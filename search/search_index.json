{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"iicd-workshop-2024","text":"<p>Code accompanying the iicd workshop 2024.</p> <p>Problem set: problem_set.md</p>"},{"location":"problem_set/","title":"Problem set 3 - Probabilistic modeling","text":"<p>Wednesday, 2024-07-10</p> <p>In this problem set, we explore probabilistic modeling on scRNA-sequencing data.</p> <p>We recommend starting a new conda environment for this problem set. <pre><code>conda create -n iicd-workshop python=3.11\nconda activate iicd-workshop\n</code></pre></p> <p>Before starting, make sure the <code>iicd-workshop-2024</code> package is installed. <pre><code>pip install iicd-workshop-2024\n</code></pre></p> <p>As well as other dependencies: <pre><code>pip install scvi-tools torch scanpy seaborn matplotlib\n</code></pre></p>"},{"location":"problem_set/#problem-1-simple-model-with-global-gene-parameters","title":"Problem 1 - Simple model with global gene parameters","text":"<p>In this problem, you will implement simple models with global gene parameters. For each cell \\(i\\) and gene \\(g\\), assume that the gene expression \\(x_{i,g}\\) follows a distribution $$ x_{i,g} \\sim p(\\theta_g) $$ where \\(\\theta_g\\) contains gene specific parameters shared across cells, and \\(p\\) is a distribution (e.g. Normal, Negative Binomial).</p> <p>You will implement simple gene model by subclassing the <code>BaseGeneModel</code> class provided in the <code>iicd_workshop_2024</code> package.</p> <p>Link to the documentation: BaseGeneModel Note: the instance function <code>fit</code> simply calls a helper function, fit, which loads data from the AnnData object and runs a training loop. This is already implemented for you.</p> <p>You can use the Distribution classes from <code>torch.distributions</code>. Available here: torch.distributions.</p>"},{"location":"problem_set/#1-load-the-data","title":"1) Load the data","text":"<p>Load public scRNA-seq data <code>pbmc</code> from the <code>scvi-tools</code> package. <pre><code>import scvi\n\nadata = scvi.data.pbmc_dataset()\n</code></pre></p>"},{"location":"problem_set/#2-implement-a-simple-normal-gene-model","title":"2) Implement a simple Normal gene model","text":"<p>In this question, assume that $$ x_{i,g} \\sim \\mathcal{N}(\\mu_g, \\sigma_g^2) $$ where \\(\\mu_g\\) and \\(\\sigma_g\\) are gene specific parameters shared across cells.</p> <p>Subclass the <code>BaseGeneModel</code> class to implement Normal gene model.</p> <pre><code>import torch.distributions as dist\nfrom iicd_workshop_2024.gene_model import BaseGeneModel\n\n\nclass NormalGeneModel(BaseGeneModel):\n    def get_mean(self, gene_idx=None):\n        return ...\n\n    def get_std(self, gene_idx=None):\n        return ...\n\n    def get_distribution(self, gene_idx=None) -&gt; dist.Distribution:\n        return ...\n</code></pre>"},{"location":"problem_set/#21-implement-the-get_mean-method","title":"2.1) Implement the <code>get_mean</code> method","text":"<p>Look at the documentation for what the get_mean method should return.</p>"},{"location":"problem_set/#22-implement-the-get_std-method","title":"2.2) Implement the <code>get_std</code> method","text":"<p>Look at the documentation for the get_std method should return.</p>"},{"location":"problem_set/#23-implement-the-get_distribution-method","title":"2.3) Implement the <code>get_distribution</code> method","text":"<p>You should use the <code>dist.Normal</code> class from <code>torch.distributions</code> to create the distribution object.</p>"},{"location":"problem_set/#24-fit-the-model","title":"2.4) Fit the model","text":"<p>Fit the model to the data using the <code>fit</code> method. We recommend you to read the documentation and source code for the fit method.</p>"},{"location":"problem_set/#25-visualize-the-learned-means","title":"2.5) Visualize the learned means","text":"<p>Visualize the learned gene means against the true empirical gene means.</p> <ul> <li>Are they the same?</li> <li>If not, can you hypothesize reasons for any differences?</li> <li>And can you fix it?</li> </ul> <p>To visualize the gene means, you can use the following code snippet: <pre><code>import seaborn as sns\nimport matplotlib.pyplot as plt\n\n\ndef plot_learned_vs_empirical_mean(model, adata):\n    empirical_means = ...\n    learned_means = ...\n    sns.scatterplot(x=empirical_means, y=learned_means)\n    max_value = max(empirical_means.max(), learned_means.max())\n    plt.plot([0, max_value], [0, max_value], color='black', linestyle='--')\n    plt.xlabel(\"Empirical mean\")\n    plt.ylabel(\"Learned mean\")\n    plt.show()\n</code></pre></p>"},{"location":"problem_set/#26-visualize-a-few-gene-distributions","title":"2.6) Visualize a few gene distributions","text":"<p>Visualize a few gene distributions by plotting the learned distributions against the empirical distribution. For example, genes: <code>[\"CD14\", \"CD74\", \"RPS27\"]</code>.</p> <p>You can use the function <code>plot_gene_distribution</code> from the <code>iicd_workshop_2024.gene_model</code> module. See documentation for plot_gene_distribution.</p>"},{"location":"problem_set/#3-now-repeat-the-same-steps-for-a-poisson-gene-model","title":"3) Now repeat the same steps for a Poisson gene model","text":"<p>Math reminder:</p> <ul> <li>The Poisson distribution is a distribution over non-negative integers.</li> <li>It is parametrized by a single parameter \\(\\lambda\\), its mean (which is also its variance). $$ x \\sim \\text{Poisson}(\\lambda) $$ with \\(\\mathbb{E}[x] = \\text{Var}[x] = \\lambda\\).</li> </ul>"},{"location":"problem_set/#4-now-repeat-the-same-steps-for-a-negative-binomial-gene-model","title":"4) Now repeat the same steps for a Negative Binomial gene model","text":"<p>Math reminder:</p> <ul> <li>The Negative Binomial distribution is a distribution over non-negative integers.</li> <li>It is parametrized by two parameters: the mean \\(\\mu\\) and the inverse dispersion parameter \\(\\alpha\\). $$ x \\sim \\text{NegBinom}(\\mu, \\alpha) $$ with $ \\mathbb{E}[x] = \\mu$ and \\(\\text{Var}[x] = \\mu + \\frac{\\mu^2}{\\alpha}\\).</li> <li>The Negative Binomial distribution implemented in pytorch does not use the mean and inverse dispersion parameter.   Instead, it uses the <code>total_count</code> and <code>logit</code> parameters.   The correspondance is given by:</li> <li>total_count = inverse dispersion</li> <li>logit = log(mean) - log(inverse dispersion)</li> </ul>"},{"location":"problem_set/#problem-2-gene-model-with-cell-specific-parameters","title":"Problem 2 - Gene model with cell specific parameters","text":"<p>In this problem, you will implement gene models with cell specific parameters, learned with an autoencoder.</p> <p>In the previous problem, we assumed that the gene expression \\(x_{i,g}\\) was distributed according to: $$ x_{i,g} \\sim p(\\theta_g) $$ where \\(\\theta_g\\) contains gene specific parameters shared across cells. In reality, the gene expression is influenced by cell specific parameters as well.</p> <p>The idea of representation learning for high-dimensional data is that we might not know exactly what are the cell specific parameters that influence the gene expression, but we assume that there exist a small number of them.</p> <p>We write \\(z_i\\) the cell specific representation and we obtain the model: $$ \\begin{align} x_{i,g} &amp;\\sim p(f(z_i), \\theta_g) \\end{align} $$ where:</p> <ul> <li>\\(f\\) is a function that transforms the cell specific representation into the gene specific parameters.</li> </ul> <p>For this problem, we define:</p> <ul> <li>the family of distributions \\(p\\) to be negative binomial distributions</li> <li>the prior distribution of the gene specific parameters \\(\\rho\\) to be uniform</li> <li>the prior distribution of the cell specific representations \\(\\pi\\) to be a standard normal distribution.</li> </ul> <p>We further will use amortized inference to learn the cell specific representations \\(z_i\\). $$ z_i \\approx g(x_i), $$ where \\(g\\) is a neural network that takes the gene expression \\(x_i\\) as input and outputs the cell specific representation \\(z_i\\).</p>"},{"location":"problem_set/#1-implement-the-auto-encoder-model","title":"1) Implement the auto-encoder model","text":"<p>Implement the auto-encoder model by completing the following class. You may use the fit function to train the model as is done in the BaseGeneModel. <pre><code>import torch\n\n\nclass LatentModel(torch.nn.Module):\n    def __init__(self, n_genes: int, n_latent: int):\n        super().__init__()\n        ...\n\n    def forward(self, x):\n       ...\n\n    def loss(self, data):\n        return -self.forward(data).log_prob(data).mean()\n</code></pre></p> <p>You can also use the DenseNN class from the <code>iicd_workshop_2024.neural_network</code> module to define the neural network. <pre><code>from iicd_workshop_2024.neural_network import DenseNN\n</code></pre></p>"},{"location":"problem_set/#2-fit-the-auto-encoder-model","title":"2) Fit the auto-encoder model","text":"<p>You can use the <code>fit</code> function from the <code>iicd_workshop_2024.inference</code> module to fit the model.</p>"},{"location":"problem_set/#3-implement-a-get_latent_representation-method","title":"3) Implement a <code>get_latent_representation</code> method","text":"<p>This function should be able to retrieve the latent vectors <code>z</code> for any given <code>x</code> input.</p>"},{"location":"problem_set/#4-visualize-the-learned-cell-specific-representations-using-umap","title":"4) Visualize the learned cell specific representations using UMAP","text":"<p>You can use <code>scanpy</code> to visualize the learned cell specific representations using UMAP. Does the latent space appear coherent? Can you validate whether the latent space preserves any prior annotations expected to dominate the signal?</p>"},{"location":"problem_set/#5-compare-against-decipher","title":"5) Compare against Decipher","text":"<p>We would now like to see how our simple autoencoder model compares to Decipher.</p> <p>Install <code>decipher</code> by running: <pre><code>pip install scdecipher\n</code></pre></p> <p>Decipher has an API close to <code>scanpy</code>, with computational function in <code>decipher.tl</code> and plotting functions in <code>decipher.pl</code>. You can fit a Decipher model to the data using the following code snippet: <pre><code>import decipher as dc\n\n# color by cell type (called 'str_labels' in the pbmc dataset)\nmodel = dc.tl.decipher_train(adata, plot_every_k_epochs=1, plot_kwargs=dict(color='str_labels'))\n</code></pre></p> <p>You should be able to train the model and retrieve a similar latent representation. Visualize this representation and compare it against the one from your autoencoder.</p> <p>How do they differ?</p> <p>Optional:</p> <ul> <li>Decode a series of points across a data-dense region of the latent representations of the auto-encoder and Decipher.</li> <li>Then, for each gene (or a select few genes), plot the trend in gene expression values corresponding to the series of points.</li> <li>Do they appear smooth or discontinuous?</li> <li>Are there correlations between certain genes?</li> <li>Are there sudden shifts in gene expression that correspond with annotation changes?</li> </ul>"},{"location":"problem_set/#problem-3-implement-your-own-model","title":"Problem 3 - Implement your own model","text":"<p>Now you can implement your own model on your own data. Good luck!</p>"},{"location":"references/","title":"API Reference","text":""},{"location":"references/#iicd_workshop_2024.gene_model.BaseGeneModel","title":"<code>BaseGeneModel</code>","text":"<p>             Bases: <code>ABC</code>, <code>Module</code></p> <p>Base class for modeling the expression of gene expression using distributions with gene-specific parameters that are shared across cells.</p> <p>Parameters:</p> Name Type Description Default <code>n_genes</code> <code>int</code> <p>The number of genes to model.</p> required <p>Attributes:</p> Name Type Description <code>_mean</code> <code>Parameter</code> <p>The mean parameter for each gene.</p> <code>_std</code> <code>Parameter</code> <p>The standard deviation parameter for each gene.</p> <code>_inverse_dispersion</code> <code>Parameter</code> <p>The inverse dispersion parameter for each gene.</p> Source code in <code>iicd_workshop_2024/gene_model.py</code> <pre><code>class BaseGeneModel(abc.ABC, torch.nn.Module):\n    \"\"\"\n    Base class for modeling the expression of gene expression using distributions with\n    gene-specific parameters that are shared across cells.\n\n    Args:\n        n_genes (int): The number of genes to model.\n\n    Attributes:\n        _mean (torch.nn.Parameter): The mean parameter for each gene.\n        _std (torch.nn.Parameter): The standard deviation parameter for each gene.\n        _inverse_dispersion (torch.nn.Parameter): The inverse dispersion parameter for each gene.\n    \"\"\"\n\n    def __init__(self, n_genes):\n        super().__init__()\n        self._mean = torch.nn.Parameter(torch.randn(n_genes))\n        self._std = torch.nn.Parameter(torch.randn(n_genes))\n        self._inverse_dispersion = torch.nn.Parameter(torch.randn(n_genes))\n\n    @property\n    def distribution_name(self):\n        \"\"\"\n        Get the name of the distribution used for modeling the gene expression.\n        \"\"\"\n        return self.get_distribution().__class__.__name__.lower()\n\n    def get_mean(self, gene_idx=None) -&gt; torch.Tensor | list[torch.Tensor]:\n        \"\"\"\n        Get the mean parameter of the distributions of gene.\n        The method is used for Gaussian, Poisson, and negative binomial distributions.\n\n        Args:\n            gene_idx (int or list[int] or None): If None, return the mean parameter of all genes.\n                Otherwise, return the mean parameter of the specified gene or list of genes (given by their indices).\n\n        Returns:\n            torch.Tensor or list[torch.Tensor]: The mean parameter of the distribution(s) of the gene(s).\n        \"\"\"\n        raise NotImplementedError\n\n    def get_std(self, gene_idx=None) -&gt; torch.Tensor | list[torch.Tensor]:\n        \"\"\"\n        Get the standard deviation parameter of the distributions of gene.\n        The method is used for Gaussian distributions.\n\n        Args:\n            gene_idx (int or list[int] or None): If None, return the standard deviation parameter of all genes.\n                Otherwise, return the standard deviation parameter of the specified gene or list of genes (given by their indices).\n\n        Returns:\n            torch.Tensor or list[torch.Tensor]: The standard deviation parameter of the distribution(s) of the gene(s).\n\n        \"\"\"\n        raise NotImplementedError\n\n    def get_inverse_dispersion(self, gene_idx=None) -&gt; torch.Tensor | list[torch.Tensor]:\n        \"\"\"\n        Get the inverse dispersion parameter of the distributions of gene.\n        The method is used for negative binomial distributions.\n\n        Args:\n            gene_idx (int or list[int] or None): If None, return the inverse dispersion parameter of all genes.\n                Otherwise, return the inverse dispersion parameter of the specified gene or list of genes (given by their indices).\n\n        Returns:\n            torch.Tensor or list[torch.Tensor]: The inverse dispersion parameter of the distribution(s) of the gene(s).\n\n        \"\"\"\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def get_distribution(self, gene_idx=None) -&gt; dist.Distribution:\n        \"\"\"\n        Get the distribution that models the gene expression.\n\n        Args:\n            gene_idx (int or list[int] or None): If None, return the distribution over all genes. Otherwise, return the distribution\n                of the specified gene or list of genes (given by their indices).\n\n        Returns:\n            dist.Distribution or list[dist.Distribution]: The distribution(s) of the gene(s).\n        \"\"\"\n        pass\n\n    def loss(self, data) -&gt; torch.Tensor:\n        \"\"\"\n        Return the negative log-likelihood of the data given the model.\n        Args:\n            data (torch.Tensor): The observations on which to compute the negative log-likelihood.\n\n        Returns:\n            torch.Tensor: The negative log-likelihood of the data given the model.\n        \"\"\"\n        return -self.get_distribution().log_prob(data).mean()\n\n    def fit(self, adata, epochs=100, batch_size=128, lr=1e-2):\n        \"\"\"\n        Fit the model to the data.\n        **NOTE: No need to override this method. It is implemented for you.**\n\n        Args:\n            adata (AnnData): Annotated data matrix.\n            epochs (int): Number of epochs to train the model.\n            batch_size (int): Batch size.\n            lr (float): Learning rate.\n        \"\"\"\n        fit(self, adata, epochs=epochs, batch_size=batch_size, lr=lr)\n</code></pre>"},{"location":"references/#iicd_workshop_2024.gene_model.BaseGeneModel.distribution_name","title":"<code>distribution_name</code>  <code>property</code>","text":"<p>Get the name of the distribution used for modeling the gene expression.</p>"},{"location":"references/#iicd_workshop_2024.gene_model.BaseGeneModel.fit","title":"<code>fit(adata, epochs=100, batch_size=128, lr=0.01)</code>","text":"<p>Fit the model to the data. NOTE: No need to override this method. It is implemented for you.</p> <p>Parameters:</p> Name Type Description Default <code>adata</code> <code>AnnData</code> <p>Annotated data matrix.</p> required <code>epochs</code> <code>int</code> <p>Number of epochs to train the model.</p> <code>100</code> <code>batch_size</code> <code>int</code> <p>Batch size.</p> <code>128</code> <code>lr</code> <code>float</code> <p>Learning rate.</p> <code>0.01</code> Source code in <code>iicd_workshop_2024/gene_model.py</code> <pre><code>def fit(self, adata, epochs=100, batch_size=128, lr=1e-2):\n    \"\"\"\n    Fit the model to the data.\n    **NOTE: No need to override this method. It is implemented for you.**\n\n    Args:\n        adata (AnnData): Annotated data matrix.\n        epochs (int): Number of epochs to train the model.\n        batch_size (int): Batch size.\n        lr (float): Learning rate.\n    \"\"\"\n    fit(self, adata, epochs=epochs, batch_size=batch_size, lr=lr)\n</code></pre>"},{"location":"references/#iicd_workshop_2024.gene_model.BaseGeneModel.get_distribution","title":"<code>get_distribution(gene_idx=None)</code>  <code>abstractmethod</code>","text":"<p>Get the distribution that models the gene expression.</p> <p>Parameters:</p> Name Type Description Default <code>gene_idx</code> <code>int or list[int] or None</code> <p>If None, return the distribution over all genes. Otherwise, return the distribution of the specified gene or list of genes (given by their indices).</p> <code>None</code> <p>Returns:</p> Type Description <code>Distribution</code> <p>dist.Distribution or list[dist.Distribution]: The distribution(s) of the gene(s).</p> Source code in <code>iicd_workshop_2024/gene_model.py</code> <pre><code>@abc.abstractmethod\ndef get_distribution(self, gene_idx=None) -&gt; dist.Distribution:\n    \"\"\"\n    Get the distribution that models the gene expression.\n\n    Args:\n        gene_idx (int or list[int] or None): If None, return the distribution over all genes. Otherwise, return the distribution\n            of the specified gene or list of genes (given by their indices).\n\n    Returns:\n        dist.Distribution or list[dist.Distribution]: The distribution(s) of the gene(s).\n    \"\"\"\n    pass\n</code></pre>"},{"location":"references/#iicd_workshop_2024.gene_model.BaseGeneModel.get_inverse_dispersion","title":"<code>get_inverse_dispersion(gene_idx=None)</code>","text":"<p>Get the inverse dispersion parameter of the distributions of gene. The method is used for negative binomial distributions.</p> <p>Parameters:</p> Name Type Description Default <code>gene_idx</code> <code>int or list[int] or None</code> <p>If None, return the inverse dispersion parameter of all genes. Otherwise, return the inverse dispersion parameter of the specified gene or list of genes (given by their indices).</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor | list[Tensor]</code> <p>torch.Tensor or list[torch.Tensor]: The inverse dispersion parameter of the distribution(s) of the gene(s).</p> Source code in <code>iicd_workshop_2024/gene_model.py</code> <pre><code>def get_inverse_dispersion(self, gene_idx=None) -&gt; torch.Tensor | list[torch.Tensor]:\n    \"\"\"\n    Get the inverse dispersion parameter of the distributions of gene.\n    The method is used for negative binomial distributions.\n\n    Args:\n        gene_idx (int or list[int] or None): If None, return the inverse dispersion parameter of all genes.\n            Otherwise, return the inverse dispersion parameter of the specified gene or list of genes (given by their indices).\n\n    Returns:\n        torch.Tensor or list[torch.Tensor]: The inverse dispersion parameter of the distribution(s) of the gene(s).\n\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"references/#iicd_workshop_2024.gene_model.BaseGeneModel.get_mean","title":"<code>get_mean(gene_idx=None)</code>","text":"<p>Get the mean parameter of the distributions of gene. The method is used for Gaussian, Poisson, and negative binomial distributions.</p> <p>Parameters:</p> Name Type Description Default <code>gene_idx</code> <code>int or list[int] or None</code> <p>If None, return the mean parameter of all genes. Otherwise, return the mean parameter of the specified gene or list of genes (given by their indices).</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor | list[Tensor]</code> <p>torch.Tensor or list[torch.Tensor]: The mean parameter of the distribution(s) of the gene(s).</p> Source code in <code>iicd_workshop_2024/gene_model.py</code> <pre><code>def get_mean(self, gene_idx=None) -&gt; torch.Tensor | list[torch.Tensor]:\n    \"\"\"\n    Get the mean parameter of the distributions of gene.\n    The method is used for Gaussian, Poisson, and negative binomial distributions.\n\n    Args:\n        gene_idx (int or list[int] or None): If None, return the mean parameter of all genes.\n            Otherwise, return the mean parameter of the specified gene or list of genes (given by their indices).\n\n    Returns:\n        torch.Tensor or list[torch.Tensor]: The mean parameter of the distribution(s) of the gene(s).\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"references/#iicd_workshop_2024.gene_model.BaseGeneModel.get_std","title":"<code>get_std(gene_idx=None)</code>","text":"<p>Get the standard deviation parameter of the distributions of gene. The method is used for Gaussian distributions.</p> <p>Parameters:</p> Name Type Description Default <code>gene_idx</code> <code>int or list[int] or None</code> <p>If None, return the standard deviation parameter of all genes. Otherwise, return the standard deviation parameter of the specified gene or list of genes (given by their indices).</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor | list[Tensor]</code> <p>torch.Tensor or list[torch.Tensor]: The standard deviation parameter of the distribution(s) of the gene(s).</p> Source code in <code>iicd_workshop_2024/gene_model.py</code> <pre><code>def get_std(self, gene_idx=None) -&gt; torch.Tensor | list[torch.Tensor]:\n    \"\"\"\n    Get the standard deviation parameter of the distributions of gene.\n    The method is used for Gaussian distributions.\n\n    Args:\n        gene_idx (int or list[int] or None): If None, return the standard deviation parameter of all genes.\n            Otherwise, return the standard deviation parameter of the specified gene or list of genes (given by their indices).\n\n    Returns:\n        torch.Tensor or list[torch.Tensor]: The standard deviation parameter of the distribution(s) of the gene(s).\n\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"references/#iicd_workshop_2024.gene_model.BaseGeneModel.loss","title":"<code>loss(data)</code>","text":"<p>Return the negative log-likelihood of the data given the model. Args:     data (torch.Tensor): The observations on which to compute the negative log-likelihood.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The negative log-likelihood of the data given the model.</p> Source code in <code>iicd_workshop_2024/gene_model.py</code> <pre><code>def loss(self, data) -&gt; torch.Tensor:\n    \"\"\"\n    Return the negative log-likelihood of the data given the model.\n    Args:\n        data (torch.Tensor): The observations on which to compute the negative log-likelihood.\n\n    Returns:\n        torch.Tensor: The negative log-likelihood of the data given the model.\n    \"\"\"\n    return -self.get_distribution().log_prob(data).mean()\n</code></pre>"},{"location":"references/#iicd_workshop_2024.gene_model.plot_gene_distribution","title":"<code>plot_gene_distribution(model, adata, genes, n_cols=3)</code>","text":"<p>Plot the learned distributions and the empirical distributions of the genes.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>BaseGeneModel</code> <p>The gene model.</p> required <code>adata</code> <code>AnnData</code> <p>The annotated data matrix.</p> required <code>genes</code> <code>list[str]</code> <p>The list of genes to plot.</p> required <code>n_cols</code> <code>int</code> <p>The number of columns in the plot.</p> <code>3</code> Source code in <code>iicd_workshop_2024/gene_model.py</code> <pre><code>def plot_gene_distribution(model: BaseGeneModel, adata, genes, n_cols=3):\n    \"\"\"\n    Plot the learned distributions and the empirical distributions of the genes.\n\n    Args:\n        model (BaseGeneModel): The gene model.\n        adata (AnnData): The annotated data matrix.\n        genes (list[str]): The list of genes to plot.\n        n_cols (int): The number of columns in the plot.\n    \"\"\"\n    n_rows = int(np.ceil(len(genes) / n_cols))\n    fig, axs = plt.subplots(n_rows, n_cols, figsize=(n_cols * 4, n_rows * 3), squeeze=False)\n    for i, gene in enumerate(genes):\n        ax = axs[i // n_cols, i % n_cols]\n        gene_idx = adata.var[\"gene_symbols\"].tolist().index(gene)\n        sns.histplot(adata.X[:, gene_idx].toarray(), stat=\"density\", discrete=True, ax=ax)\n        max_value = adata.X[:, gene_idx].max().item()\n        if model.distribution_name in [\"poisson\", \"negativebinomial\"]:\n            x = torch.arange(0, max_value + 1)\n        else:\n            x = torch.linspace(\n                min(-5, model.get_mean(gene_idx) - 2 * model.get_std(gene_idx)), max_value, 1000\n            )\n        y = model.get_distribution(gene_idx).log_prob(x).exp().detach().numpy()\n        sns.lineplot(x=x, y=y, ax=ax, color=\"red\")\n        ax.set_title(gene + f\" (idx={gene_idx})\")\n    plt.tight_layout()\n    plt.show()\n</code></pre>"},{"location":"references/#iicd_workshop_2024.inference.fit","title":"<code>fit(model, adata, epochs=100, batch_size=128, lr=0.01)</code>","text":"<p>Fit the model to the data.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The model to fit.</p> required <code>adata</code> <code>AnnData</code> <p>The annotated data matrix.</p> required <code>epochs</code> <code>int</code> <p>Number of epochs to train the model.</p> <code>100</code> <code>batch_size</code> <code>int</code> <p>Batch size.</p> <code>128</code> <code>lr</code> <code>float</code> <p>Learning rate.</p> <code>0.01</code> Source code in <code>iicd_workshop_2024/inference.py</code> <pre><code>def fit(model, adata, epochs=100, batch_size=128, lr=1e-2):\n    \"\"\"\n    Fit the model to the data.\n\n    Args:\n        model (nn.Module): The model to fit.\n        adata (AnnData): The annotated data matrix.\n        epochs (int): Number of epochs to train the model.\n        batch_size (int): Batch size.\n        lr (float): Learning rate.\n    \"\"\"\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    data_X = adata.X\n    # check if sparse\n    if isinstance(data_X, scipy.sparse.csr_matrix):\n        data_X = data_X.toarray()\n    data_loader = torch.utils.data.DataLoader(data_X, batch_size=batch_size, shuffle=True)\n    pbar = tqdm.tqdm(total=epochs * len(data_loader))\n    for _ in range(epochs):\n        for x in data_loader:\n            optimizer.zero_grad()\n            loss = model.loss(x).mean()\n            loss.backward()\n            optimizer.step()\n            pbar.set_postfix(loss=loss.item())\n            pbar.update()\n    pbar.close()\n</code></pre>"},{"location":"references/#iicd_workshop_2024.neural_network.DenseNN","title":"<code>DenseNN</code>","text":"<p>             Bases: <code>Module</code></p> <p>A simple feedforward neural network with ReLU activation function.</p> <p>Parameters:</p> Name Type Description Default <code>n_input</code> <code>int</code> <p>The number of input features.</p> required <code>n_output</code> <code>int</code> <p>The number of output features.</p> required <code>n_hidden</code> <code>int</code> <p>The number of hidden units in each hidden layer.</p> <code>128</code> <code>n_layers</code> <code>int</code> <p>The number of hidden layers.</p> <code>1</code> Source code in <code>iicd_workshop_2024/neural_network.py</code> <pre><code>class DenseNN(torch.nn.Module):\n    \"\"\"\n    A simple feedforward neural network with ReLU activation function.\n\n    Args:\n        n_input (int): The number of input features.\n        n_output (int): The number of output features.\n        n_hidden (int): The number of hidden units in each hidden layer.\n        n_layers (int): The number of hidden layers.\n    \"\"\"\n\n    def __init__(self, n_input: int, n_output: int, n_hidden: int = 128, n_layers: int = 1):\n        super().__init__()\n        self.layers = torch.nn.ModuleList()\n        self.layers.append(torch.nn.Linear(n_input, n_hidden))\n        for _ in range(n_layers - 1):\n            self.layers.append(torch.nn.Linear(n_hidden, n_hidden))\n        self.layers.append(torch.nn.Linear(n_hidden, n_output))\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass of the neural network.\n        Args:\n            x (torch.Tensor): The input tensor.\n\n        Returns:\n            torch.Tensor: The output tensor.\n        \"\"\"\n        for layer in self.layers[:-1]:\n            x = torch.relu(layer(x))\n        return self.layers[-1](x)\n</code></pre>"},{"location":"references/#iicd_workshop_2024.neural_network.DenseNN.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass of the neural network. Args:     x (torch.Tensor): The input tensor.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The output tensor.</p> Source code in <code>iicd_workshop_2024/neural_network.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass of the neural network.\n    Args:\n        x (torch.Tensor): The input tensor.\n\n    Returns:\n        torch.Tensor: The output tensor.\n    \"\"\"\n    for layer in self.layers[:-1]:\n        x = torch.relu(layer(x))\n    return self.layers[-1](x)\n</code></pre>"}]}