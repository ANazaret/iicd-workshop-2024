{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"iicd-workshop-2024","text":"<p>Code accompanying the iicd workshop 2024.</p> <p>Problem set: problem_set.md</p>"},{"location":"problem_set/","title":"Problem set 3 - Probabilistic modeling","text":"<p>Wednesday, 2024-07-10</p> <p>In this problem set, we explore probabilistic modeling on scRNA-sequencing data.</p> <p>Before starting, make sure the <code>iicd-workshop-2024</code> package is installed. <pre><code>pip install iicd-workshop-2024\n</code></pre></p>"},{"location":"problem_set/#problem-1-simple-model-with-global-gene-parameters","title":"Problem 1 - Simple model with global gene parameters","text":"<p>In this problem, you will implement simple models with global gene parameters. For each cell \\(i\\) and gene \\(g\\), assume that the gene expression \\(x_{i,g}\\) follows a distribution $$ x_{i,g} \\sim p(\\theta_g) $$ where \\(\\theta_g\\) contains gene specific parameters shared across cells, and \\(p\\) is a distribution (e.g. Normal, Negative Binomial).</p> <p>You will implement simple gene model by subclassing the <code>BaseGeneModel</code> class provided in the <code>iicd_workshop_2024</code> package.</p> <p>Link to the documentation: BaseGeneModel Note: the instance function <code>fit</code> simply calls a helper function, fit, which loads data from the AnnData object and runs a training loop. This is already implemented for you.</p>"},{"location":"problem_set/#1-load-the-data","title":"1) Load the data","text":"<p>Load public scRNA-seq data <code>pbmc</code> from the <code>scvi-tools</code> package. <pre><code>import scvi\n\nadata = scvi.data.pbmc_dataset()\n</code></pre></p>"},{"location":"problem_set/#2-implement-a-simple-normal-gene-model","title":"2) Implement a simple Normal gene model","text":"<p>In this question, assume that $$ x_{i,g} \\sim \\mathcal{N}(\\mu_g, \\sigma_g^2) $$ where \\(\\mu_g\\) and \\(\\sigma_g\\) are gene specific parameters shared across cells.</p> <p>Subclass the <code>BaseGeneModel</code> class to implement Normal gene model.</p> <pre><code>import torch.distributions as dist\nfrom iicd_workshop_2024.gene_model import BaseGeneModel\n\n\nclass NormalGeneModel(BaseGeneModel):\n    def get_mean(self, gene_idx=None):\n        if gene_idx is None:\n            gene_idx = slice(None)\n        return ...\n\n    def get_std(self, gene_idx=None):\n        if gene_idx is None:\n            gene_idx = slice(None)\n        return ...\n\n    def get_distribution(self, gene_idx=None) -&gt; dist.Distribution:\n        return ...\n</code></pre>"},{"location":"problem_set/#21-implement-the-get_mean-method","title":"2.1) Implement the <code>get_mean</code> method","text":""},{"location":"problem_set/#22-implement-the-get_std-method","title":"2.2) Implement the <code>get_std</code> method","text":""},{"location":"problem_set/#23-implement-the-get_distribution-method","title":"2.3) Implement the <code>get_distribution</code> method","text":"<p>You should use the <code>dist.Normal</code> class from <code>torch.distributions</code> to create the distribution.</p>"},{"location":"problem_set/#24-fit-the-model","title":"2.4) Fit the model","text":"<p>Fit the model to the data using the <code>fit</code> method.</p>"},{"location":"problem_set/#25-visualize-the-learned-means","title":"2.5) Visualize the learned means","text":"<p>Visualize the learned gene means against the true empirical gene means.</p> <ul> <li>Are they the same?</li> <li>If not, can you hypothesize reasons for any differences?</li> <li>And can you fix it?</li> </ul> <p>To visualize the gene means, you can use the following code snippet: <pre><code>import seaborn as sns\nimport matplotlib.pyplot as plt\n\n\ndef plot_learned_vs_empirical_mean(model, adata):\n    empirical_means = ...\n    learned_means = ...\n    sns.scatterplot(x=empirical_means, y=learned_means)\n    max_value = max(empirical_means.max(), learned_means.max())\n    plt.plot([0, max_value], [0, max_value], color='black', linestyle='--')\n    plt.xlabel(\"Empirical mean\")\n    plt.ylabel(\"Learned mean\")\n    plt.show()\n</code></pre></p>"},{"location":"problem_set/#26-visualize-a-few-gene-distributions","title":"2.6) Visualize a few gene distributions","text":"<p>Visualize a few gene distributions by plotting the learned distributions against the empirical distribution.</p> <p>You can use the function <code>plot_gene_distribution</code> from the <code>iicd_workshop_2024.gene_model</code> module. See documentation for plot_gene_distribution.</p>"},{"location":"problem_set/#3-now-repeat-the-same-steps-for-a-poisson-gene-model","title":"3) Now repeat the same steps for a Poisson gene model","text":"<p>Math reminder:</p> <ul> <li>The Poisson distribution is a distribution over non-negative integers.</li> <li>It is parametrized by a single parameter \\(\\lambda\\), its mean (which is also its variance). $$ x \\sim \\text{Poisson}(\\lambda) $$ with \\(\\mathbb{E}[x] = \\text{Var}[x] = \\lambda\\).</li> </ul>"},{"location":"problem_set/#4-now-repeat-the-same-steps-for-a-negative-binomial-gene-model","title":"4) Now repeat the same steps for a Negative Binomial gene model","text":"<p>Math reminder:</p> <ul> <li>The Negative Binomial distribution is a distribution over non-negative integers.</li> <li>It is parametrized by two parameters: the mean \\(\\mu\\) and the inverse dispersion parameter \\(\\alpha\\). $$ x \\sim \\text{NegBinom}(\\mu, \\alpha) $$ with $ \\mathbb{E}[x] = \\mu$ and \\(\\text{Var}[x] = \\mu + \\frac{\\mu^2}{\\alpha}\\).</li> <li>The Negative Binomial distribution implemented in pytorch does not use the mean and inverse dispersion parameter.   Instead, it uses the <code>total_count</code> and <code>logit</code> parameters.   The correspondance is given by:</li> <li>total_count = inverse dispersion</li> <li>logit = log(mean) - log(inverse dispersion)</li> </ul>"},{"location":"problem_set/#problem-2-gene-model-with-cell-specific-parameters","title":"Problem 2 - Gene model with cell specific parameters","text":"<p>In this problem, you will implement gene models with cell specific parameters, learned with an autoencoder.</p> <p>In the previous problem, we assumed that the gene expression \\(x_{i,g}\\) was distributed according to: $$ x_{i,g} \\sim p(\\theta_g) $$ where \\(\\theta_g\\) contains gene specific parameters shared across cells. In reality, the gene expression is influenced by cell specific parameters as well.</p> <p>The idea of representation learning for high-dimensional data is that we might not know exactly what are the cell specific parameters that influence the gene expression, but we assume that there exist a small number of them.</p> <p>We write \\(z_i\\) the cell specific representation and we obtain the model: $$ \\begin{align} x_{i,g} &amp;\\sim p(f(z_i), \\theta_g) \\end{align} $$ $$ \\begin{align} z_i \\sim \\pi, \\quad \\theta_g \\sim \\rho \\end{align} $$ where:</p> <ul> <li>\\(\\pi\\) is the prior distribution of the cell specific representations,</li> <li>\\(\\rho\\) is the prior distribution of the gene specific parameters</li> <li>\\(f\\) is a function that transforms the cell specific representation into the gene specific parameters.</li> </ul> <p>For this problem, we define:</p> <ul> <li>the family of distributions \\(p\\) to be negative binomial distributions</li> <li>the prior distribution of the gene specific parameters \\(\\rho\\) to be uniform</li> <li>the prior distribution of the cell specific representations \\(\\pi\\) to be a standard normal distribution.</li> </ul> <p>We further will use amortized inference to learn the cell specific representations \\(z_i\\). $$ z_i \\approx g(x_i), $$ where \\(g\\) is a neural network that takes the gene expression \\(x_i\\) as input and outputs the cell specific representation \\(z_i\\).</p>"},{"location":"problem_set/#1-implement-the-auto-encoder-model","title":"1) Implement the auto-encoder model","text":"<p>Implement the auto-encoder model by completing the following class. You may use the fit function to train the model as is done in the BaseGeneModel. <pre><code>import torch\n\n\nclass LatentModel(torch.nn.Module):\n    def __init__(self, n_genes: int, n_latent: int):\n        super().__init__()\n        ...\n\n    def forward(self, x):\n       ...\n\n    def loss(self, data):\n        return -self.forward(data).log_prob(data).mean()\n</code></pre></p>"},{"location":"problem_set/#2-fit-the-auto-encoder-model","title":"2) Fit the auto-encoder model","text":"<p>You can use the <code>fit</code> function from the <code>iicd_workshop_2024.inference</code> module to fit the model.</p>"},{"location":"problem_set/#3-implement-a-get_latent_representation-method","title":"3) Implement a <code>get_latent_representation</code> method","text":"<p>This function should be able to retrieve the latent vectors <code>z</code> for any given <code>x</code> input.</p>"},{"location":"problem_set/#4-visualize-the-learned-cell-specific-representations-using-umap","title":"4) Visualize the learned cell specific representations using UMAP","text":"<p>You can use <code>scanpy</code> to visualize the learned cell specific representations using UMAP. Does the latent space appear coherent? Can you validate whether the latent space preserves any prior annotations expected to dominate the signal?</p>"},{"location":"problem_set/#5-compare-against-decipher","title":"5) Compare against Decipher","text":"<p>We would now like to see how our simple autoencoder model compares to Decipher.</p> <p>Follow instructions here to install Decipher. You should be able to train the model and retrieve a similar latent representation. Visualize this representation and compare it against the one from your autoencoder. How do they differ?</p> <p>Optional:</p> <ul> <li>Decode a series of points across a data-dense region of the latent representations of the auto-encoder and Decipher.</li> <li>Then, for each gene (or a select few genes), plot the trend in gene expression values corresponding to the series of points.</li> <li>Do they appear smooth or discontinuous?</li> <li>Are there correlations between certain genes?</li> <li>Are there sudden shifts in gene expression that correspond with annotation changes?</li> </ul>"},{"location":"problem_set/#problem-3-implement-your-own-model","title":"Problem 3 - Implement your own model","text":"<p>Now you can implement your own model on your own data. Good luck!</p>"},{"location":"references/","title":"API Reference","text":""},{"location":"references/#iicd_workshop_2024.gene_model.BaseGeneModel","title":"<code>BaseGeneModel</code>","text":"<p>             Bases: <code>ABC</code>, <code>Module</code></p> <p>Base class for modeling the expression of gene expression using distributions with gene-specific parameters that are shared across cells.</p> Source code in <code>iicd_workshop_2024/gene_model.py</code> <pre><code>class BaseGeneModel(abc.ABC, torch.nn.Module):\n    \"\"\"\n    Base class for modeling the expression of gene expression using distributions with\n    gene-specific parameters that are shared across cells.\n    \"\"\"\n\n    def __init__(self, n_genes):\n        super().__init__()\n        self._mean = torch.nn.Parameter(torch.randn(n_genes))\n        self._std = torch.nn.Parameter(torch.randn(n_genes))\n        self._inverse_dispersion = torch.nn.Parameter(torch.randn(n_genes))\n\n    @property\n    def distribution_name(self):\n        \"\"\"\n        Get the name of the distribution used for modeling the gene expression.\n        \"\"\"\n        return self.get_distribution().__class__.__name__.lower()\n\n    def get_mean(self, gene_idx=None):\n        \"\"\"\n        Get the mean parameter of the distributions of gene.\n        The method is used for Gaussian, Poisson, and negative binomial distributions.\n\n        Parameters\n        ----------\n        gene_idx: int or list[int] or None\n            If None, return the mean parameter of all genes. Otherwise, return the mean parameter\n            of the specified gene or list of genes (given by their indices).\n\n        Returns\n        -------\n        torch.Tensor or list[torch.Tensor]\n            The mean parameter of the distribution(s) of the gene(s).\n        \"\"\"\n        raise NotImplementedError\n\n    def get_std(self, gene_idx=None):\n        \"\"\"\n        Get the standard deviation parameter of the distributions of gene.\n        The method is used for Gaussian distributions.\n\n        Parameters\n        ----------\n        gene_idx: int or list[int] or None\n            If None, return the standard deviation parameter of all genes. Otherwise, return the standard deviation parameter\n            of the specified gene or list of genes (given by their indices).\n\n        Returns\n        -------\n        torch.Tensor or list[torch.Tensor]\n            The standard deviation parameter of the distribution(s) of the gene(s).\n\n        \"\"\"\n        raise NotImplementedError\n\n    def get_inverse_dispersion(self, gene_idx=None):\n        \"\"\"\n        Get the inverse dispersion parameter of the distributions of gene.\n        The method is used for negative binomial distributions.\n\n        Parameters\n        ----------\n        gene_idx: int or list[int] or None\n            If None, return the inverse dispersion parameter of all genes. Otherwise, return the inverse dispersion parameter\n            of the specified gene or list of genes (given by their indices).\n\n        Returns\n        -------\n        torch.Tensor or list[torch.Tensor]\n            The inverse dispersion parameter of the distribution(s) of the gene(s).\n\n        \"\"\"\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def get_distribution(self, gene_idx=None) -&gt; dist.Distribution:\n        \"\"\"\n        Get the distribution that models the gene expression.\n\n        Parameters\n        ----------\n        gene_idx: int or list[int] or None\n            If None, return the distribution over all genes. Otherwise, return the distribution\n            of the specified gene or list of genes (given by their indices).\n\n        Returns\n        -------\n        dist.Distribution or list[dist.Distribution]\n            The distribution(s) of the gene(s).\n        \"\"\"\n        pass\n\n    def loss(self, data) -&gt; torch.Tensor:\n        \"\"\"\n        Return the negative log-likelihood of the data given the model.\n\n        Parameters\n        ----------\n        data: torch.Tensor\n            The observations on which to compute the negative log-likelihood.\n\n        Returns\n        -------\n        torch.Tensor\n            The negative log-likelihood of the data given the model.\n        \"\"\"\n        return -self.get_distribution().log_prob(data).mean()\n\n    def fit(self, adata, epochs=100, batch_size=128, lr=1e-2):\n        \"\"\"\n        Fit the model to the data.\n        **NOTE: No need to override this method. It is implemented for you.**\n\n        Parameters\n        ----------\n        adata: AnnData\n            Annotated data matrix.\n        epochs: int\n            Number of epochs to train the model.\n        batch_size: int\n            Batch size.\n        lr: float\n            Learning rate.\n        \"\"\"\n        fit(self, adata, epochs=epochs, batch_size=batch_size, lr=lr)\n</code></pre>"},{"location":"references/#iicd_workshop_2024.gene_model.BaseGeneModel.distribution_name","title":"<code>distribution_name</code>  <code>property</code>","text":"<p>Get the name of the distribution used for modeling the gene expression.</p>"},{"location":"references/#iicd_workshop_2024.gene_model.BaseGeneModel.fit","title":"<code>fit(adata, epochs=100, batch_size=128, lr=0.01)</code>","text":"<p>Fit the model to the data. NOTE: No need to override this method. It is implemented for you.</p>"},{"location":"references/#iicd_workshop_2024.gene_model.BaseGeneModel.fit--parameters","title":"Parameters","text":"<p>adata: AnnData     Annotated data matrix. epochs: int     Number of epochs to train the model. batch_size: int     Batch size. lr: float     Learning rate.</p> Source code in <code>iicd_workshop_2024/gene_model.py</code> <pre><code>def fit(self, adata, epochs=100, batch_size=128, lr=1e-2):\n    \"\"\"\n    Fit the model to the data.\n    **NOTE: No need to override this method. It is implemented for you.**\n\n    Parameters\n    ----------\n    adata: AnnData\n        Annotated data matrix.\n    epochs: int\n        Number of epochs to train the model.\n    batch_size: int\n        Batch size.\n    lr: float\n        Learning rate.\n    \"\"\"\n    fit(self, adata, epochs=epochs, batch_size=batch_size, lr=lr)\n</code></pre>"},{"location":"references/#iicd_workshop_2024.gene_model.BaseGeneModel.get_distribution","title":"<code>get_distribution(gene_idx=None)</code>  <code>abstractmethod</code>","text":"<p>Get the distribution that models the gene expression.</p>"},{"location":"references/#iicd_workshop_2024.gene_model.BaseGeneModel.get_distribution--parameters","title":"Parameters","text":"<p>gene_idx: int or list[int] or None     If None, return the distribution over all genes. Otherwise, return the distribution     of the specified gene or list of genes (given by their indices).</p>"},{"location":"references/#iicd_workshop_2024.gene_model.BaseGeneModel.get_distribution--returns","title":"Returns","text":"<p>dist.Distribution or list[dist.Distribution]     The distribution(s) of the gene(s).</p> Source code in <code>iicd_workshop_2024/gene_model.py</code> <pre><code>@abc.abstractmethod\ndef get_distribution(self, gene_idx=None) -&gt; dist.Distribution:\n    \"\"\"\n    Get the distribution that models the gene expression.\n\n    Parameters\n    ----------\n    gene_idx: int or list[int] or None\n        If None, return the distribution over all genes. Otherwise, return the distribution\n        of the specified gene or list of genes (given by their indices).\n\n    Returns\n    -------\n    dist.Distribution or list[dist.Distribution]\n        The distribution(s) of the gene(s).\n    \"\"\"\n    pass\n</code></pre>"},{"location":"references/#iicd_workshop_2024.gene_model.BaseGeneModel.get_inverse_dispersion","title":"<code>get_inverse_dispersion(gene_idx=None)</code>","text":"<p>Get the inverse dispersion parameter of the distributions of gene. The method is used for negative binomial distributions.</p>"},{"location":"references/#iicd_workshop_2024.gene_model.BaseGeneModel.get_inverse_dispersion--parameters","title":"Parameters","text":"<p>gene_idx: int or list[int] or None     If None, return the inverse dispersion parameter of all genes. Otherwise, return the inverse dispersion parameter     of the specified gene or list of genes (given by their indices).</p>"},{"location":"references/#iicd_workshop_2024.gene_model.BaseGeneModel.get_inverse_dispersion--returns","title":"Returns","text":"<p>torch.Tensor or list[torch.Tensor]     The inverse dispersion parameter of the distribution(s) of the gene(s).</p> Source code in <code>iicd_workshop_2024/gene_model.py</code> <pre><code>def get_inverse_dispersion(self, gene_idx=None):\n    \"\"\"\n    Get the inverse dispersion parameter of the distributions of gene.\n    The method is used for negative binomial distributions.\n\n    Parameters\n    ----------\n    gene_idx: int or list[int] or None\n        If None, return the inverse dispersion parameter of all genes. Otherwise, return the inverse dispersion parameter\n        of the specified gene or list of genes (given by their indices).\n\n    Returns\n    -------\n    torch.Tensor or list[torch.Tensor]\n        The inverse dispersion parameter of the distribution(s) of the gene(s).\n\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"references/#iicd_workshop_2024.gene_model.BaseGeneModel.get_mean","title":"<code>get_mean(gene_idx=None)</code>","text":"<p>Get the mean parameter of the distributions of gene. The method is used for Gaussian, Poisson, and negative binomial distributions.</p>"},{"location":"references/#iicd_workshop_2024.gene_model.BaseGeneModel.get_mean--parameters","title":"Parameters","text":"<p>gene_idx: int or list[int] or None     If None, return the mean parameter of all genes. Otherwise, return the mean parameter     of the specified gene or list of genes (given by their indices).</p>"},{"location":"references/#iicd_workshop_2024.gene_model.BaseGeneModel.get_mean--returns","title":"Returns","text":"<p>torch.Tensor or list[torch.Tensor]     The mean parameter of the distribution(s) of the gene(s).</p> Source code in <code>iicd_workshop_2024/gene_model.py</code> <pre><code>def get_mean(self, gene_idx=None):\n    \"\"\"\n    Get the mean parameter of the distributions of gene.\n    The method is used for Gaussian, Poisson, and negative binomial distributions.\n\n    Parameters\n    ----------\n    gene_idx: int or list[int] or None\n        If None, return the mean parameter of all genes. Otherwise, return the mean parameter\n        of the specified gene or list of genes (given by their indices).\n\n    Returns\n    -------\n    torch.Tensor or list[torch.Tensor]\n        The mean parameter of the distribution(s) of the gene(s).\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"references/#iicd_workshop_2024.gene_model.BaseGeneModel.get_std","title":"<code>get_std(gene_idx=None)</code>","text":"<p>Get the standard deviation parameter of the distributions of gene. The method is used for Gaussian distributions.</p>"},{"location":"references/#iicd_workshop_2024.gene_model.BaseGeneModel.get_std--parameters","title":"Parameters","text":"<p>gene_idx: int or list[int] or None     If None, return the standard deviation parameter of all genes. Otherwise, return the standard deviation parameter     of the specified gene or list of genes (given by their indices).</p>"},{"location":"references/#iicd_workshop_2024.gene_model.BaseGeneModel.get_std--returns","title":"Returns","text":"<p>torch.Tensor or list[torch.Tensor]     The standard deviation parameter of the distribution(s) of the gene(s).</p> Source code in <code>iicd_workshop_2024/gene_model.py</code> <pre><code>def get_std(self, gene_idx=None):\n    \"\"\"\n    Get the standard deviation parameter of the distributions of gene.\n    The method is used for Gaussian distributions.\n\n    Parameters\n    ----------\n    gene_idx: int or list[int] or None\n        If None, return the standard deviation parameter of all genes. Otherwise, return the standard deviation parameter\n        of the specified gene or list of genes (given by their indices).\n\n    Returns\n    -------\n    torch.Tensor or list[torch.Tensor]\n        The standard deviation parameter of the distribution(s) of the gene(s).\n\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"references/#iicd_workshop_2024.gene_model.BaseGeneModel.loss","title":"<code>loss(data)</code>","text":"<p>Return the negative log-likelihood of the data given the model.</p>"},{"location":"references/#iicd_workshop_2024.gene_model.BaseGeneModel.loss--parameters","title":"Parameters","text":"<p>data: torch.Tensor     The observations on which to compute the negative log-likelihood.</p>"},{"location":"references/#iicd_workshop_2024.gene_model.BaseGeneModel.loss--returns","title":"Returns","text":"<p>torch.Tensor     The negative log-likelihood of the data given the model.</p> Source code in <code>iicd_workshop_2024/gene_model.py</code> <pre><code>def loss(self, data) -&gt; torch.Tensor:\n    \"\"\"\n    Return the negative log-likelihood of the data given the model.\n\n    Parameters\n    ----------\n    data: torch.Tensor\n        The observations on which to compute the negative log-likelihood.\n\n    Returns\n    -------\n    torch.Tensor\n        The negative log-likelihood of the data given the model.\n    \"\"\"\n    return -self.get_distribution().log_prob(data).mean()\n</code></pre>"},{"location":"references/#iicd_workshop_2024.gene_model.plot_gene_distribution","title":"<code>plot_gene_distribution(model, adata, genes, n_cols=3)</code>","text":"<p>Plot the learned distributions and the empirical distributions of the genes.</p>"},{"location":"references/#iicd_workshop_2024.gene_model.plot_gene_distribution--parameters","title":"Parameters","text":"<p>model: BaseGeneModel     The gene model. adata: AnnData     The annotated data matrix. genes: list[str]     The list of genes to plot. n_cols: int     The number of columns in the plot.</p> Source code in <code>iicd_workshop_2024/gene_model.py</code> <pre><code>def plot_gene_distribution(model: BaseGeneModel, adata, genes, n_cols=3):\n    \"\"\"\n    Plot the learned distributions and the empirical distributions of the genes.\n\n    Parameters\n    ----------\n    model: BaseGeneModel\n        The gene model.\n    adata: AnnData\n        The annotated data matrix.\n    genes: list[str]\n        The list of genes to plot.\n    n_cols: int\n        The number of columns in the plot.\n    \"\"\"\n    n_rows = int(np.ceil(len(genes) / n_cols))\n    fig, axs = plt.subplots(n_rows, n_cols, figsize=(n_cols * 4, n_rows * 3), squeeze=False)\n    for i, gene in enumerate(genes):\n        ax = axs[i // n_cols, i % n_cols]\n        gene_idx = adata.var[\"gene_symbols\"].tolist().index(gene)\n        sns.histplot(adata.X[:, gene_idx].toarray(), stat=\"density\", discrete=True, ax=ax)\n        max_value = adata.X[:, gene_idx].max().item()\n        if model.distribution_name in [\"poisson\", \"negativebinomial\"]:\n            x = torch.arange(0, max_value + 1)\n        else:\n            x = torch.linspace(\n                min(-5, model.get_mean(gene_idx) - 2 * model.get_std(gene_idx)), max_value, 1000\n            )\n        y = model.get_distribution(gene_idx).log_prob(x).exp().detach().numpy()\n        sns.lineplot(x=x, y=y, ax=ax, color=\"red\")\n        ax.set_title(gene + f\" (idx={gene_idx})\")\n    plt.tight_layout()\n    plt.show()\n</code></pre>"},{"location":"references/#iicd_workshop_2024.inference.fit","title":"<code>fit(model, adata, epochs=100, batch_size=128, lr=0.01)</code>","text":"<p>Fit the model to the data.</p>"},{"location":"references/#iicd_workshop_2024.inference.fit--parameters","title":"Parameters","text":"<p>model:     The model to fit. adata: AnnData     The annotated data matrix. epochs: int     Number of epochs to train the model. batch_size: int     Batch size. lr: float     Learning rate.</p> Source code in <code>iicd_workshop_2024/inference.py</code> <pre><code>def fit(model, adata, epochs=100, batch_size=128, lr=1e-2):\n    \"\"\"\n    Fit the model to the data.\n\n    Parameters\n    ----------\n    model:\n        The model to fit.\n    adata: AnnData\n        The annotated data matrix.\n    epochs: int\n        Number of epochs to train the model.\n    batch_size: int\n        Batch size.\n    lr: float\n        Learning rate.\n    \"\"\"\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    data_X = adata.X\n    # check if sparse\n    if isinstance(data_X, scipy.sparse.csr_matrix):\n        data_X = data_X.toarray()\n    data_loader = torch.utils.data.DataLoader(data_X, batch_size=batch_size, shuffle=True)\n    pbar = tqdm.tqdm(total=epochs * len(data_loader))\n    for _ in range(epochs):\n        for x in data_loader:\n            optimizer.zero_grad()\n            loss = model.loss(x).mean()\n            loss.backward()\n            optimizer.step()\n            pbar.set_postfix(loss=loss.item())\n            pbar.update()\n    pbar.close()\n</code></pre>"}]}